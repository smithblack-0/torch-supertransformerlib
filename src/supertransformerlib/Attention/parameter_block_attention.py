"""

Parameter Injection Attention

Allows for large blocks of parameters to be elegantly inserted into a
model
"""

import torch
from torch import nn
from src.supertransformerlib import Core
from src.supertransformerlib.Attention import Utility
from typing import Optional

class ParameterInjectionException(Core.ValidationError):
    def __init__(self, reason: str):
        super().__init__("ParameterInjectionException", reason)


class _ParameterInjectionAttention:
    """
    The primary virtual layer. The implimentation
    for the parameter injection attention mechanism.

    Designed to be generated by a factory class.
    """

    def __init__(self,
                 static_keys: torch.Tensor,
                 static_values: torch.Tensor,
                 QueryHeader: Utility.MakeHeadFactory.Type,
                 Deheader: Utility.RemoveHeads,
                 ):
        self.make_query_head = QueryHeader
        self.remove_heads = Deheader
        self.keys = static_keys
        self.values = static_values

    def __call__(self, query: torch.Tensor)->torch.Tensor:

        headed_query = self.make_query_head(query)
        attn = Utility.dot_product_attention(headed_query,
                                             self.keys,
                                             self.values)
        output = self.remove_heads(attn)
        return output

class ParameterInjectionAttentionFactory(nn.Module):
    """
    The factory class for this process.

    This consists of replacing the key and value
    blocks of attention with blocks of parameters,
    thereby giving a model the ability to see a
    pattern and recall the same response. In a
    sense it acts as a sort of calibration.

    The number of choices each parameter bank
    possesses is the bank size.
    """
    def __init__(self,
                 d_model: int,
                 heads: int,
                 bank_size: int,
                 parallel: Core.StandardShapeType,
                 dtype: Optional[torch.dtype] = None,
                 device: Optional[torch.device] = None,
                 ):

        super().__init__()

        if parallel is not None:
            parallel = Core.standardize_shape(parallel, "parallel")

        d_head = d_model // heads
        if d_head < 2:
            reason = f"""\
            You specified a 'd_model' embedding size of {d_model} 
            and a number of heads equal to {heads}. This results in 
            a head embedding width of {d_head} which is too small
            to make logical decisions. Increase d_model or decrease 
            heads.
            """
            reason = Core.dedent(reason)
            raise ParameterInjectionException(reason)

        # Create the parameter banks. These banks should be
        # shape [heads, bank_size, d_head] and consist of the keys
        # and values needed to perform attention with the headed
        # query.

        keys = torch.empty([heads, bank_size, d_head], dtype=dtype, device=device)
        torch.nn.init.kaiming_uniform_(keys)

        values = torch.empty([heads, bank_size, d_head], dtype=dtype, device=device)
        torch.nn.init.kaiming_uniform_(values)

        # Store away everything

        self.queryHeaderFactory = Utility.MakeHeadFactory(d_model, heads, d_head,
                                                   parallel, dtype, device)
        self.deHeaderFactory = Utility.RemoveHeadsFactory(d_model, heads, d_head,
                                                          parallel, dtype, device)
        self.keys = keys
        self.values = values

    def forward(self)->_ParameterInjectionAttention:
        queryHeader = self.queryHeaderFactory()
        deheader = self.deHeaderFactory()

        return _ParameterInjectionAttention(self.keys,
                                            self.values,
                                            queryHeader,
                                            deheader)





