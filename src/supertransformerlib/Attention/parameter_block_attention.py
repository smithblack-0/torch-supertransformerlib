"""

Parameter block attention

Allows for large blocks of parameters to be elegantly inserted into a
model
"""

import torch
from torch import nn
from src.supertransformerlib import Core
from src.supertransformerlib.Attention import Utility
from typing import Optional

class ParameterInjectionException(Core.ValidationError):
    def __init__(self, reason: str):
        super().__init__("ParameterInjectionException", reason)


class _ParameterBlockAttention:
    """
    The primary virtual layer. The implimentation
    for the parameter injection attention mechanism.

    Designed to be generated by a factory class.
    """

    def __init__(self,
                 static_keys: torch.Tensor,
                 static_values: torch.Tensor,
                 make_query_head: Utility.MakeHeadFactory.Type,
                 merge_heads: Utility.RemoveHeads,
                 ):
        self.make_query_head = make_query_head
        self.merge_heads = merge_heads
        self.keys = static_keys
        self.values = static_values

    def __call__(self, query: torch.Tensor)->torch.Tensor:

        headed_query = self.make_query_head(query)
        attn = Utility.dot_product_attention(headed_query,
                                             self.keys,
                                             self.values)
        output = self.merge_heads(attn)
        return output

class ParameterBlockAttentionFactory(nn.Module):
    """
    The factory class for this process.

    This consists of replacing the key and value
    blocks of attention with blocks of parameters,
    thereby giving a model the ability to see a
    pattern and recall the same response. In a
    sense it acts as a sort of calibration.

    The number of choices each parameter bank
    possesses is the bank size.
    """
    def __init__(self,
                 d_model: int,
                 heads: int,
                 block_size: int,
                 parallel: Core.StandardShapeType,
                 dtype: Optional[torch.dtype] = None,
                 device: Optional[torch.device] = None,
                 ):

        super().__init__()

        if parallel is not None:
            parallel = Core.standardize_shape(parallel, "parallel")

        d_head = d_model // heads
        if d_head < 2:
            reason = f"""\
            You specified a 'd_model' embedding size of {d_model} 
            and a number of heads equal to {heads}. This results in 
            a head embedding width of {d_head} which is too small
            to make logical decisions. Increase d_model or decrease 
            heads.
            """
            reason = Core.dedent(reason)
            raise ParameterInjectionException(reason)

        # Create the parameter banks. These banks should be
        # shape [heads, bank_size, d_head] and consist of the keys
        # and values needed to perform attention with the headed
        # query.

        keys = torch.empty([heads, block_size, d_head], dtype=dtype, device=device)
        torch.nn.init.kaiming_uniform_(keys)

        values = torch.empty([heads, block_size, d_head], dtype=dtype, device=device)
        torch.nn.init.kaiming_uniform_(values)

        # Store away everything

        self.make_query_head_factory = Utility.MakeHeadFactory(d_model, heads, d_head,
                                                   parallel, dtype, device)
        self.merge_heads_factory = Utility.RemoveHeadsFactory(d_model, heads, d_head,
                                                          parallel, dtype, device)
        self.keys = keys
        self.values = values

    def forward(self)->_ParameterBlockAttention:
        make_query_head = self.make_query_head_factory()
        merge_heads = self.merge_heads_factory()

        return _ParameterBlockAttention(self.keys,
                                        self.values,
                                        make_query_head,
                                        merge_heads)





